import torch
import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.datasets import CocoDetection
from torch.utils.data import DataLoader, random_split
from torchvision.transforms import ToTensor, Compose, RandomHorizontalFlip
from pycocotools.coco import COCO
import os
import zipfile
import urllib.request
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import time

# --- 0. é…ç½®å‚æ•° (Configuration) ---
COCO_DATA_ROOT = './coco_data' 
NUM_CLASSES = 91 # COCO 80 categories + 1 background + others
DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
BATCH_SIZE = 4
LEARNING_RATE = 0.005
NUM_EPOCHS = 5 # å»ºè®®åœ¨æ­£å¼æäº¤å‰è®­ç»ƒæ›´ä¹… (ä¾‹å¦‚ 20+ epochs)
SAVE_PATH = 'faster_rcnn_coco_assignment_weights.pth' 


# --- 1. æ•°æ®ä¸‹è½½ä¸è®¾ç½® (Data Preprocessing Requirement 1) ---

class DownloadProgressBar(tqdm):
    """å¸¦è¿›åº¦æ¡çš„ä¸‹è½½å·¥å…·"""
    def update_to(self, b=1, bsize=1, tsize=None):
        if tsize is not None:
            self.total = tsize
        self.update(b * bsize - self.n)

def download_url(url, output_path):
    """å¸¦è¿›åº¦æ¡çš„ä¸‹è½½å‡½æ•°"""
    with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:
        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)

def setup_coco_dataset():
    """ä¸‹è½½å¹¶è§£å‹ COCO 2017 è®­ç»ƒé›†å›¾ç‰‡å’Œæ ‡æ³¨æ–‡ä»¶"""
    print("--- æ­£åœ¨è®¾ç½® COCO æ•°æ®é›† (ç”¨äºè®­ç»ƒ) ---")
    os.makedirs(COCO_DATA_ROOT, exist_ok=True)
    
    files_to_download = {
        'train2017.zip': 'http://images.cocodataset.org/zips/train2017.zip',
        'annotations_trainval2017.zip': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip'
    }

    for filename, url in files_to_download.items():
        zip_path = os.path.join(COCO_DATA_ROOT, filename)
        
        # ç®€åŒ–æ£€æŸ¥ï¼Œåªè¦ç›®æ ‡ç›®å½•å­˜åœ¨ä¸”éç©ºå°±è·³è¿‡
        if filename == 'annotations_trainval2017.zip':
            target_dir = os.path.join(COCO_DATA_ROOT, 'annotations')
        else:
            target_dir = os.path.join(COCO_DATA_ROOT, filename.split('.')[0])
        
        if os.path.exists(target_dir) and os.listdir(target_dir):
            print(f"âœ… {filename.split('.')[0]} å·²å­˜åœ¨ã€‚è·³è¿‡ä¸‹è½½å’Œè§£å‹ã€‚")
            continue
                
        # ä¸‹è½½å’Œè§£å‹é€»è¾‘ (ä¸ä¹‹å‰ä»£ç ç›¸åŒ)
        if not os.path.exists(zip_path):
            print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½ {filename} (è¯·è€å¿ƒç­‰å¾…)...")
            download_url(url, zip_path)
            print(f"âœ… {filename} ä¸‹è½½å®Œæˆã€‚")
        
        print(f"ğŸ”¨ æ­£åœ¨è§£å‹ {filename}...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(COCO_DATA_ROOT)
        print(f"âœ… {filename} è§£å‹å®Œæˆã€‚")
    
    IMG_DIR_TRAIN = os.path.join(COCO_DATA_ROOT, 'train2017')
    ANN_FILE_TRAIN = os.path.join(COCO_DATA_ROOT, 'annotations', 'instances_train2017.json')
    
    print("--- COCO æ•°æ®é›†è®¾ç½®å®Œæˆ ---")
    return IMG_DIR_TRAIN, ANN_FILE_TRAIN


# --- 2. æ•°æ®é›†ç±»å’ŒåŠ è½½å™¨ (Data Preprocessing) ---

class CocoDetection_Custom(CocoDetection):
    """
    é‡å†™ CocoDetectionï¼Œå°† COCO æ ¼å¼æ ‡æ³¨è½¬æ¢ä¸º PyTorch ç›®æ ‡æ£€æµ‹æ¨¡å‹æ‰€éœ€çš„æ ¼å¼ã€‚
    """
    def __init__(self, root, annFile, transform=None):
        super().__init__(root, annFile, transform)
        # è·å– COCO API å®ä¾‹
        self.coco = COCO(annFile)
        self.ids = list(sorted(self.coco.imgs.keys()))
        
    def __getitem__(self, index):
        img_id = self.ids[index]
        ann_ids = self.coco.getAnnIds(imgIds=img_id)
        coco_target = self.coco.loadAnns(ann_ids)
        image = self._load_image(img_id)

        boxes = []
        labels = []
        
        for annotation in coco_target:
            if annotation.get('iscrowd', 0) == 1: # å¿½ç•¥ iscrowd ç›®æ ‡
                continue
            x, y, w, h = annotation['bbox']
            boxes.append([x, y, x + w, y + h]) # è½¬æ¢ä¸º [x_min, y_min, x_max, y_max]
            labels.append(annotation['category_id']) 
        
        # è½¬æ¢ä¸º Tensor æ ¼å¼
        if not boxes:
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros((0,), dtype=torch.int64)
        else:
            boxes = torch.as_tensor(boxes, dtype=torch.float32)
            labels = torch.as_tensor(labels, dtype=torch.int64)

        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = torch.tensor([img_id])
        target["area"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        target["iscrowd"] = torch.zeros((len(boxes),), dtype=torch.int64)
        
        # æ•°æ®å¢å¼º/è½¬æ¢
        if self.transform is not None:
             image = self.transform(image)

        return image, target

def collate_fn(batch):
    """ç”¨äº DataLoader çš„è‡ªå®šä¹‰ collate function"""
    return tuple(zip(*batch))

# --- 3. æ¨¡å‹è®¾è®¡ (Model Design Requirement 2) ---

def get_faster_rcnn_model(num_classes):
    """
    ä½¿ç”¨é¢„è®­ç»ƒçš„ ResNet50-FPN ä½œä¸ºä¸»å¹²ç½‘ç»œçš„ Faster R-CNN æ¨¡å‹ã€‚
    """
    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights="DEFAULT")

    # æ›¿æ¢åˆ†ç±»å™¨å¤´éƒ¨ä»¥é€‚åº”æ–°çš„ç±»åˆ«æ•°
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    return model

# --- 4. è®­ç»ƒå’Œä¼˜åŒ–å‡½æ•° (Training Requirement 3) ---

def train_one_epoch(model, optimizer, data_loader, device, epoch):
    model.train()
    running_loss = 0.0
    
    print(f"\n--- Epoch {epoch} Start ---")
    
    # ä½¿ç”¨ tqdm åŒ…è£… data_loader ä»¥æ˜¾ç¤ºè¿›åº¦
    data_iterator = tqdm(data_loader, desc=f"Epoch {epoch} Training")
    
    for i, (images, targets) in enumerate(data_iterator):
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼ä¸‹è®¡ç®—æŸå¤± (åˆ†ç±»æŸå¤± + å›å½’æŸå¤±)
        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())
        
        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        running_loss += losses.item()
        
        data_iterator.set_postfix({'Loss': f'{running_loss/(i+1):.4f}'})

    print(f"Epoch {epoch} Final Loss: {running_loss/len(data_loader):.4f}")


# --- 5. å¯è§†åŒ–å’Œè¯„ä¼°å‡½æ•° (Evaluation and Analysis Requirement 4 & Visualization) ---

def visualize_sample(dataset, index, num_samples=1, is_ground_truth=True):
    """
    å¯è§†åŒ–æ ·æœ¬å›¾ç‰‡çš„ Ground Truth æ ‡æ³¨æˆ–æ¨¡å‹é¢„æµ‹ã€‚
    (Requirement 1 & 4)
    """
    if num_samples > 3: num_samples = 3 # é™åˆ¶æ•°é‡
    
    for i in range(num_samples):
        # ä½¿ç”¨ä¸å¸¦å¢å¼ºçš„åŸå§‹å›¾ç‰‡æ¥ä¿è¯å¯è§†åŒ–æ­£ç¡®
        image, target = dataset[index + i] 
        
        # å°†å›¾ç‰‡ä» Tensor è½¬æ¢ä¸º PIL Image æˆ– Numpy Array
        img = (image * 255).byte().permute(1, 2, 0).cpu().numpy()
        fig, ax = plt.subplots(1)
        ax.imshow(img)

        boxes = target['boxes'].cpu().numpy()
        labels = target['labels'].cpu().numpy()
        
        title = "Ground Truth Annotation"
        if not is_ground_truth:
             title = "Model Prediction"

        print(f"Image {index + i} {title} (ID: {target['image_id'].item()})")
        
        for box, label in zip(boxes, labels):
            x_min, y_min, x_max, y_max = box
            width = x_max - x_min
            height = y_max - y_min
            
            # ç»˜åˆ¶çŸ©å½¢æ¡†
            rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')
            ax.add_patch(rect)
            ax.text(x_min, y_min - 5, f'Class ID: {label}', color='white', backgroundcolor='red')

        plt.title(title)
        plt.show()

def evaluate_coco(model, data_loader, device):
    """
    ä½¿ç”¨ pycocotools è¿›è¡Œ COCO mAP æ ‡å‡†è¯„ä¼° (Requirement 4)ã€‚
    æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªç®€åŒ–éª¨æ¶ã€‚åœ¨å®é™…æäº¤æ—¶ï¼Œä½ éœ€è¦ä½¿ç”¨ PyTorch å®˜æ–¹
    æ£€æµ‹ç¤ºä¾‹ (references/detection) ä¸­çš„ engine.py å’Œ coco_eval.py
    è„šæœ¬ï¼Œå®ƒä»¬åŒ…å«äº†å®Œæ•´çš„ COCO è¯„ä¼°é€»è¾‘ã€‚
    """
    print("--- æ­£åœ¨è¿›è¡Œ COCO æ ‡å‡†è¯„ä¼° ---")
    model.eval()
    
    # COCO è¯„ä¼°éœ€è¦ä¸€ä¸ªç‰¹æ®Šçš„æ ¼å¼ï¼Œè¿™é‡Œåªå±•ç¤ºéª¨æ¶å’Œæ‰€éœ€åº“
    
    # 1. åˆå§‹åŒ– COCO å¯¹è±¡ (ç”¨äºGround Truth)
    coco_gt = data_loader.dataset.coco
    coco_dt = [] # å­˜å‚¨æ¨¡å‹é¢„æµ‹ç»“æœ

    # 2. éå†æ•°æ®å¹¶æ”¶é›†é¢„æµ‹ç»“æœ
    with torch.no_grad():
        for images, targets in tqdm(data_loader, desc="Collecting Predictions"):
            images = list(image.to(device) for image in images)
            outputs = model(images)
            
            for i, output in enumerate(outputs):
                img_id = targets[i]["image_id"].item()
                
                # è½¬æ¢é¢„æµ‹ç»“æœä¸º COCO æ ¼å¼
                boxes = output['boxes'].cpu().numpy()
                scores = output['scores'].cpu().numpy()
                labels = output['labels'].cpu().numpy()
                
                for box, score, label in zip(boxes, scores, labels):
                    if score > 0.05: # ä»…ä¿ç•™é«˜ç½®ä¿¡åº¦é¢„æµ‹
                        # è½¬æ¢ [x_min, y_min, x_max, y_max] åˆ° COCO [x, y, w, h]
                        x, y, xmax, ymax = box
                        w = xmax - x
                        h = ymax - y
                        
                        coco_dt.append({
                            "image_id": img_id,
                            "bbox": [x, y, w, h],
                            "score": score,
                            "category_id": int(label)
                        })
    
    # 3. è¿è¡Œ COCO è¯„ä¼°
    if not coco_dt:
        print("æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆç›®æ ‡ï¼Œè¯„ä¼°å¤±è´¥ã€‚")
        return

    import json
    # ä¿å­˜é¢„æµ‹ç»“æœåˆ°ä¸´æ—¶ JSON æ–‡ä»¶
    with open("results.json", "w") as f:
        json.dump(coco_dt, f)
        
    coco_results = coco_gt.loadRes("results.json")
    
    from pycocotools.cocoeval import COCOeval
    coco_eval = COCOeval(coco_gt, coco_results, 'bbox')
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()
    
    print(f"\nâœ… Mean Average Precision (mAP) for all categories: {coco_eval.stats[0]:.3f}")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    os.remove("results.json")

# --- 6. ä¸»ç¨‹åº (Main Execution) ---

def run_training_pipeline():
    # 1. è‡ªåŠ¨ä¸‹è½½å¹¶è·å–è·¯å¾„
    IMG_DIR_TRAIN, ANN_FILE_TRAIN = setup_coco_dataset()
    
    # 2. æ•°æ®åŠ è½½
    transform = Compose([
        ToTensor(), 
        RandomHorizontalFlip(0.5) # æ•°æ®å¢å¼º [cite: 8]
    ])

    dataset = CocoDetection_Custom(root=IMG_DIR_TRAIN, annFile=ANN_FILE_TRAIN, transform=transform)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.99 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, collate_fn=collate_fn
    )
    val_loader = DataLoader(
        val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, collate_fn=collate_fn
    )

    print(f"æ€»æ•°æ®é›†å¤§å°: {len(dataset)}, è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, éªŒè¯é›†å¤§å°: {len(val_dataset)}")

    # 3. æ¨¡å‹åˆå§‹åŒ–
    model = get_faster_rcnn_model(NUM_CLASSES)
    model.to(DEVICE)
    
    # 4. å¯è§†åŒ– Ground Truth æ ·æœ¬ (Requirement 1 & 4)
    # å–è®­ç»ƒé›†ä¸­çš„å‰ä¸‰ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    print("\n--- å¯è§†åŒ– Ground Truth æ ·æœ¬ (Requirement 1) ---")
    visualize_sample(train_dataset, 0, num_samples=3)

    # 5. ä¼˜åŒ–å™¨
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=0.9, weight_decay=0.0005)

    print(f"\nStart training on {len(train_dataset)} images on device: {DEVICE}")

    # 6. è®­ç»ƒ
    for epoch in range(NUM_EPOCHS):
        train_one_epoch(model, optimizer, train_loader, DEVICE, epoch)
        
        # æ¯ä¸ª epoch ç»“æŸæ—¶è¿›è¡Œè¯„ä¼° (å¯é€‰ï¼Œä½†æ¨è)
        if epoch % 1 == 0:
            evaluate_coco(model, val_loader, DEVICE)
        
    # 7. ä¿å­˜æ¨¡å‹æƒé‡ (Submission Material 2)
    torch.save(model.state_dict(), SAVE_PATH)
    print(f"\nTraining finished. Model weights saved to {SAVE_PATH}")


if __name__ == '__main__':
    run_training_pipeline()